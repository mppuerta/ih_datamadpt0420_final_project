{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/all_haiku.csv')\n",
    "\n",
    "data = data.iloc[0:1000]\n",
    "\n",
    "data['haiku'] = data['0']+' '+data['1']+' '+data['2']\n",
    "\n",
    "data['haiku'] = data['haiku'].apply(lambda x: str(x).strip().replace('-',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 50931 long\n",
      "Text has 81 unique characters\n"
     ]
    }
   ],
   "source": [
    "#Leer los datos\n",
    "\n",
    "text = ''\n",
    "\n",
    "for i in data['haiku']:\n",
    "    text += i + os.linesep\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "    \n",
    "print(f'Text is {len(text)} long')\n",
    "print(f'Text has {len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 55, 65, ..., 49, 54,  0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Procesamiento del texto\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "i\n",
      "s\n",
      "h\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fishing boats colors of the rainbow\\nash wednesday trying to remember  my dream\\nsnowy morn pouring ano'\n",
      "'ther cup of black coffee\\nshortest day flames dance in the oven\\nhaze half the horse hidden behind the '\n",
      "'house\\nlow sun the lady in red on high heels\\nadvent the passing stranger farts\\ntarn a bubble in the ic'\n",
      "\"e\\nsnowflakes new asphalt in the holes\\nCrystal Night'    gusts of rain       outside\\nrain the sound of\"\n",
      "' a horse galloping through leaves\\nwinter stars suddenly a whiff of perfume\\nhungry half of the moon hi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'fishing boats colors of the rainbow\\nash wednesday trying to remember  my dream\\nsnowy morn pouring an'\n",
      "Target data:  'ishing boats colors of the rainbow\\nash wednesday trying to remember  my dream\\nsnowy morn pouring ano'\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0\n",
      "   input: (52, \"'f'\")\n",
      "   expected output: (55, \"'i'\")\n",
      "Step     1\n",
      "   input: (55, \"'i'\")\n",
      "   expected output: (65, \"'s'\")\n",
      "Step     2\n",
      "   input: (65, \"'s'\")\n",
      "   expected output: (54, \"'h'\")\n",
      "Step     3\n",
      "   input: (54, \"'h'\")\n",
      "   expected output: (55, \"'i'\")\n",
      "Step     4\n",
      "   input: (55, \"'i'\")\n",
      "   expected output: (60, \"'n'\")\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(f'Step     {i}')\n",
    "    print(f'   input: {int(input_idx), repr(idx2char[input_idx])}')\n",
    "    print(f'   expected output: {int(target_idx), repr(idx2char[target_idx])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 81) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           20736     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 81)            83025     \n",
      "=================================================================\n",
      "Total params: 4,042,065\n",
      "Trainable params: 4,042,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67, 26,  0, 25, 22, 34, 43, 38, 66, 60,  6, 29, 28,  2, 67, 35, 75,\n",
       "       43, 65, 49, 32, 42, 39, 29, 32, 18, 56, 44,  7, 46, 28, 50, 31, 50,\n",
       "       67, 11, 80, 36,  4, 52, 40,  1, 57, 53, 60, 72, 22, 25, 71, 73, 17,\n",
       "       37, 49, 46,  9, 57, 43, 31, 51, 65, 64, 60, 28, 68, 31,  8, 13, 30,\n",
       "       41, 24, 21, 33,  8, 75, 24, 65, 39, 34, 50, 74, 51, 59, 77,  5,  3,\n",
       "       55, 51, 79, 57, 15, 20, 45, 24, 44, 48, 24, 52, 39, 38, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"seashell\\ndusk songbirds' voices disappear too\\nindian summer rouging her cheeks as she waits for a dr\"\n",
      "\n",
      "Next Char Predictions: \n",
      " 'uF\\nEBNWRtn,IH!uOéWscLVSIL;jX.ZHdKdu3…P&fT kgnzBEy~:QcZ1kWKesrnHvK06JUDAM0éDsSNd\\xa0em–\\'\"ie’k8?YDXbDfSR9'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 81)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.3937626\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 66s 9s/step - loss: 4.5262\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 73s 10s/step - loss: 3.7634\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 68s 10s/step - loss: 3.3633\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 69s 10s/step - loss: 2.9690\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 68s 10s/step - loss: 2.8336\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 68s 10s/step - loss: 2.7420\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 69s 10s/step - loss: 2.6469\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 68s 10s/step - loss: 2.5620\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 69s 10s/step - loss: 2.5014\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 67s 10s/step - loss: 2.4503\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            20736     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 81)             83025     \n",
      "=================================================================\n",
      "Total params: 4,042,065\n",
      "Trainable params: 4,042,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # using a categorical distribution to predict the character returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    # We pass the predicted character as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish  coferilc te brile theer sin he sin  ant dr'Hrerountend on yinits th. \n",
      "and sber win\n",
      "ha .ucoof tte ri slon oncpadd sur save pook cha ghent rert fan paroifrtrre sor wanthe mopf caingesm on ct fing ule d ainbp wig ofle sita s ley\n",
      "tuce hisin aldiove smeaa d rasun h\n",
      "lond baloeal og teerdind phem\n",
      "f arfiade bat . ansthperpiovesbi lit.\n",
      "— soobog par sind oushof oeg far:ige lmicer win\n",
      "ing sreron raraciig  alk of st dimm rs wiing ~ linfrerryst oft\n",
      "fhe umung sot\n",
      "wleson\n",
      "che cuye r ooftdromod bomuac alaoghe th ains lerad homondilter an fas ende lithl on hrgeen watketeslrs Io\n",
      "rume chil\n",
      " yy aduthe hatendhow mnithe s movowaitt wile thes the breg mom flofs.g thecg les koflugk singenc of ve sigl apez yasungt a yif er aof me ghintere shune mf ol bfon\n",
      "erad te il teok sowike hadis\n",
      "layar ofth gap's pthala the buthia g bopepindd ululing brog at jar iul rosk end shkescwad cus onhl thresy cuc\n",
      "lered teadneids cid bae the und at ye\n",
      "eal omkere\n",
      "rreind  ald che~ke\n",
      "tinpinordive fmek dithery won's sf merat witpirrive \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Finish \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack]",
   "language": "python",
   "name": "conda-env-ironhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
